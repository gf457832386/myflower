gpuid: 1
timestamp: ${now:%m%d%H%M}
daystamp: ${now:%m%d}
model_path: trained_models/${daystamp}/${fed_alg}-r${lora_r}-a${lora_alpha}-GPU${gpuid}-${timestamp}
results_path: results/${daystamp}/${fed_alg}-r${lora_r}-a${lora_alpha}-GPU${gpuid}-${timestamp}
output_dir: ${model_path}
base_model: roberta-base
data_path: 20newsgroup
generate_data: 0
adapter_name: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
- query
- key
- value
- dense
batch_size: 16
micro_batch_size: 4
num_epochs: 5
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 120
fed_alg: CrossLayerD
num_clients: 100
train_ratio: 0.1
data_partition_method: noniid
dirichlet_alpha: 0.5
num_rounds: 300
save_model_freq: 1
load_8bit: false
eval_step: 200
save_step: 200
use_moslora: false
use_scalelora: false
use_masklora: false
bottleneck_size: 256
non_linearity: tanh
adapter_dropout: 0.0
use_parallel_adapter: false
use_adapterp: false
scaling: 1.0
use_gradient_checkpointing: False,
num_virtual_tokens: 30,
train_on_inputs: True,
group_by_length: false
resume_from_checkpoint: None
num_cpus: 1
num_gpus: 0.1
min_fit_clients: 2
min_available_clients: 2
