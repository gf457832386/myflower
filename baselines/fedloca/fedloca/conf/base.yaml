# conf/config.yaml

defaults:
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# runtime
gpuid: 1
timestamp: ${now:%m%d%H%M}
daystamp: ${now:%m%d}
model_path: trained_models/${daystamp}/${fed_alg}-r${lora_r}-a${lora_alpha}-GPU${gpuid}-${timestamp}
results_path: results/${daystamp}/${fed_alg}-r${lora_r}-a${lora_alpha}-GPU${gpuid}-${timestamp}
output_dir: trained_models/${daystamp}/${fed_alg}-r${lora_r}-a${lora_alpha}-GPU${gpuid}-${timestamp}
seed: 42


# model and training
base_model: roberta-base
data_path: 20newsgroup
generate_data: 0
adapter_name: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: ["query", "key", "value", "dense"]

batch_size: 16
micro_batch_size: 4
num_epochs: 5
learning_rate: 3e-4
cutoff_len: 256
val_set_size: 120

# federated settings
strategy:
  _target_: fedloca.strategy.alg_fedloca.Fedloca
fed_alg: fedloca
num_clients: 100
train_ratio: 0.1
data_partition_method: noniid
dirichlet_alpha: 0.5
num_rounds: 300
save_model_freq: 1

#默认
load_8bit : False
eval_step: 200
save_step: 200
use_moslora: False
use_scalelora: False
use_masklora: False 
# bottleneck adapter hyperparams
bottleneck_size: 256
non_linearity: tanh
adapter_dropout: 0.0
use_parallel_adapter: False
use_adapterp: False
scaling:  1.0
use_gradient_checkpointing: False,
# prefix tuning hyperparams
num_virtual_tokens: 30,
# llm hyperparams
train_on_inputs: True,  # if False, masks out inputs in loss
group_by_length: False  # faster, but produces an odd training loss curve
# wandb params
# wandb_project: str = ""
# wandb_run_name: str = ""
# wandb_watch: str = ""  # options: false | gradients | all
# wandb_log_model: str = ""  # options: false | true
resume_from_checkpoint: None  # either training checkpoint or final adapter

num_cpus: 1
num_gpus: 0.1
min_fit_clients: 2
min_available_clients: 2
gradient_accumulation_steps: 4
token: ${HF_USER_TOKEN}
